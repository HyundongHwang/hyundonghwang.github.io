---
layout: post
title: 260105 책리뷰 러닝 랭체인
comments: true
tags:
---

# p43 환경구축

```bash
pip install langchain langchain-claude langchain-community
pip install langchain-text-spliiter
pip install langchain-postgres

export CLAUDE_API_KEY=xxx
```

# p44 기본 LLM 호출

```python
model = ClaudeAI(model="...")
model.invoke("하늘이")
```

```bash
푸릅니다.
```

# p45 LLM 옵션
- temperature
  - 출력생성에 사용하는 샘플링 알고리즘
  - 낮은값 : 0.1
    - 예측가능한 결과
  - 높은값 : 0.9
    - 창의적인 결과
- max_tokens
  - 출력크기와 비용제한
  - LLM이 자연스러운 마무리에 도달하기전 출력을 중단
- 역할
  - system
    - 사용자 질문에 답변하는 지시사항
  - user
    - 사용자의 쿼리
    - 사용자의 컨텐츠
  - assistant
    - 채팅모델이 생성한 컨텐츠

# p46 채팅모델 호출

```python
model = ClaudeAI("sonet...")
prompt = [HumanMsg("프랑스의 수도는 어디인가?")]
model.invoke(prompt)
```

```bash
AIMsg(content="프랑스의 수도는 파리입니다.")
```

- 채팅메시지 인터페이스
  - HumanMsg
    - 사용자 역할
    - 인간관점의 메시지
  - AIMsg
    - 어시스턴트 역할
    - AI의 관점 메시지
  - SystemMsg
    - 시스템 역할
    - AI가 준수할 지침 설정
  - ChatMsg
    - 임의의 역할

# p47 시스템 메시지 적용, 채팅모델 호출

```python
model = ChatClaudeAI()
sys_msg = SysMsg("역할 : 문장끝에 느낌표 3개를 붙여서 대답해 주세요.")
human_msg = HumanMsg("프랑스의 수도는 어디인가요?")
model.invoke([sys_msg, human_msg])
```

```bash
AIMsg("파리 입니다!!!")
```

# p48 프롬프트 템플릿

```python
template = PromptTemplate.from_template("""
    아래 작성한 context 를 기반으로 question 에 대답하세요.
    제공된 정보로 대답할 수 없는 question 이라면, "모르겠어요." 라고 대답하세요.
    
    context : {context}
    
    question : {question}
    
    answer : 
""")

prompt = template.invoke({
    "context":"""
        LLM은 NLP분야의 발전을 이끕니다.
        LLM은 작은 모델보다 우수한 성능입니다.
        개발자들은 Hugging Face의 transformer 라이브러리를 활용하거나 openai, claude의 거대모델을 활용할 수 있습니다.
    """,
    "question":"""
        거대모델은 어디서 제공하나요?
    """
})

model = ClaudeAI("sonet...")

completion = model.invoke(prompt)

print(completion)
```

```bash
Huggin Face의 transformer 라이브러리를 사용하거나 openai claude 의 라이브러리를 통해 LLM을 이용할 수 있습니다.
```

# p52 역할에 따른 동적 프롬프트

```python
template = ChatPromptTemplate.from_msg([
    ("system", """
        아래 작성한 context 를 기반으로 question 에 대답하세요.
        제공된 정보로 대답할 수 없는 question 이라면, "모르겠어요." 라고 대답하세요.
    """),
    ("human", "context:{context}"),
    ("human", "question:{question}"),
])

template.invoke({
    "context":"""
        LLM은 NLP분야의 발전을 이끕니다.
        LLM은 작은 모델보다 우수한 성능입니다.
        개발자들은 Hugging Face의 transformer 라이브러리를 활용하거나 openai, claude의 거대모델을 활용할 수 있습니다.    
    """,
    "question":"""
        거대모델은 어디서 제공하나요?
    """
})

model = ClaudeAI("sonet...")

completion = model.invoke(prompt)

print(completion)
```

```bash
AIMsg(content="Huggin Face의 transformer 라이브러리를 사용하거나 openai claude 의 라이브러리를 통해 LLM을 이용할 수 있습니다.")
```

# p56 json 형식 출력 처리 

```python
class Res(BaseModel):
    """사용자의 질문에 대한 답변과 그에 대한 근거를 함께 제공하시요."""
    
    answer:str
    """사용자의 질문에 대한 답변"""

    justification:str
    """답변에 대한 근거"""

llm = ClaudeAI(model="sonet...", temperature=0)
structured_llm = llm.with_structured_output(Res)
res = structured_llm.invoke("1킬로그램의 벽돌과 1킬로그램의 깃털 중 어느쪽이 더 무겁나요?")
print(res.model_dump_json())
```

```json
{
  "answer": "1킬로그램의 벽돌과 1킬로그램의 깃털은 동일한 무게.",
  "justification": "둘다 1킬로그램이라서 무게는 같음. 무게는 질량단위."
}
```